id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied,seltext
1b8e72j,Will Dbt just taker over the world ?,"So I started my first project on Dbt and how boy, this tool is INSANE. I just feel like any tool similar to Azure Data Factory, or Talend Cloud Platform are LIGHT-YEARS away from the power of this tool. If you think about modularity, pricing, agility, time to market, documentation, versioning, frameworks with reusability, etc. Dbt is just SO MUCH better.

If you were about to start a new cloud project, why would you not choose Fivetran/Stitch + Dbt ?",108,120,Ownards,2024-03-06 23:02:56,https://www.reddit.com/r/dataengineering/comments/1b8e72j/will_dbt_just_taker_over_the_world/,0,False,False,False,False,"So I started my first project on Dbt and how boy, this tool is INSANE. I just feel like any tool similar to Azure Data Factory, or Talend Cloud Platform are LIGHT-YEARS away from the power of this tool. If you think about modularity, pricing, agility, time to market, documentation, versioning, frameworks with reusability, etc. Dbt is just SO MUCH better.

If you were about to start a new cloud project, why would you not choose Fivetran/Stitch + Dbt ?"
1b8l1nd,"As a data engineer, what do you find the most challenging task in modern data engineering?","As a data engineer, what do you find the most challenging task in modern data engineering?",59,53,andalibansari,2024-03-07 04:10:12,https://www.reddit.com/r/dataengineering/comments/1b8l1nd/as_a_data_engineer_what_do_you_find_the_most/,0,False,False,False,False,"As a data engineer, what do you find the most challenging task in modern data engineering?"
1b8sklo,Do you think data engineering is too broad of a job title?,"During my entire career, I have had the job title of data engineer.





During my current and previous job, I worked on the software engineering side of data engineering.  Lots of work done in Python, Java, Terraform, and various cloud services.  This has definitely been my favorite part of data engineering.




However, another company I worked with involved building data pipelines in Airflow and writing lots of SQL.  I even have a friend in data engineering who mentioned that most data pipelines at his company were written in Python, manually run on his laptop ever day, and basically transformed a source excel file into an excel file that can be delivered to downstream users.",19,11,level_126_programmer,2024-03-07 11:39:31,https://www.reddit.com/r/dataengineering/comments/1b8sklo/do_you_think_data_engineering_is_too_broad_of_a/,0,False,False,False,False,"During my entire career, I have had the job title of data engineer.





During my current and previous job, I worked on the software engineering side of data engineering.  Lots of work done in Python, Java, Terraform, and various cloud services.  This has definitely been my favorite part of data engineering.




However, another company I worked with involved building data pipelines in Airflow and writing lots of SQL.  I even have a friend in data engineering who mentioned that most data pipelines at his company were written in Python, manually run on his laptop ever day, and basically transformed a source excel file into an excel file that can be delivered to downstream users."
1b8c7m3,Reneging on a job offer...,"Who's done it? What stories do you have? Preface - I just made a professional faux pas by reneging on an offer I accepted about a week ago. Totally my bad, had to apologize and have a very awkward conversation with the client company who, for the record, desperately wanted me to join their team. But upon announcing the news to my current employer that I was planning on leaving, they made me an offer I couldn't refuse. I know I know I know... never accept a counter offer!!! Well... I believe that's true most of the time but this was a unique situation and I felt like accepting the counter was definitely in my favor. I had to explain that to the company who had presented me with the offer a week previous and they went down the whole ""this puts us in a really bad spot"" (which was technically true) and ""how much more would it take for you to reconsider and join our company?"". Sometimes it's nice to be in a bidding war but it can be stressful too. Ultimately I felt like my current role was a better fit all things considered. I feel really stupid now for having accepted the other offer without first having a conversation with my current company. Lesson learned... who else has a good story?",13,15,MasterKluch,2024-03-06 21:44:56,https://www.reddit.com/r/dataengineering/comments/1b8c7m3/reneging_on_a_job_offer/,0,False,False,False,False,"Who's done it? What stories do you have? Preface - I just made a professional faux pas by reneging on an offer I accepted about a week ago. Totally my bad, had to apologize and have a very awkward conversation with the client company who, for the record, desperately wanted me to join their team. But upon announcing the news to my current employer that I was planning on leaving, they made me an offer I couldn't refuse. I know I know I know... never accept a counter offer!!! Well... I believe that's true most of the time but this was a unique situation and I felt like accepting the counter was definitely in my favor. I had to explain that to the company who had presented me with the offer a week previous and they went down the whole ""this puts us in a really bad spot"" (which was technically true) and ""how much more would it take for you to reconsider and join our company?"". Sometimes it's nice to be in a bidding war but it can be stressful too. Ultimately I felt like my current role was a better fit all things considered. I feel really stupid now for having accepted the other offer without first having a conversation with my current company. Lesson learned... who else has a good story?"
1b8kxr2,"Just created my first Data Engineering project, need the feedback!","Created a small data engineering project to test out and improve my skills, though it's not automated currently it's on my to-do list.

Tableau Dashboard- [https://public.tableau.com/app/profile/solomon8607/viz/Book1\_17097820994780/Story1](https://public.tableau.com/app/profile/solomon8607/viz/Book1_17097820994780/Story1)

Stack: Databricks - Data extraction- data extraction, cleaning and ingestion, Azure Blob storage, Azure SQL database and Tableau for visualizations.

[Architecture](https://preview.redd.it/zxo0v4e76umc1.png?width=649&format=png&auto=webp&s=c17adfff1ae82b19e82df4171e71f845bd3c83be)

Github - [https://github.com/solo11/Data-engineering-project-1](https://github.com/solo11/Data-engineering-project-1)

The project uses web-scraping to extract Buffalo, NY realty data for the last 600 days from Zillow, [Realtor.com](https://Realtor.com) and Redfin. The dashboard provides visualizations and insights into the data. 

Any feedback is much appreciated, thank you!",14,10,SnooRevelations3292,2024-03-07 04:04:49,https://www.reddit.com/r/dataengineering/comments/1b8kxr2/just_created_my_first_data_engineering_project/,0,False,False,False,False,"Created a small data engineering project to test out and improve my skills, though it's not automated currently it's on my to-do list.

Tableau Dashboard- [https://public.tableau.com/app/profile/solomon8607/viz/Book1\_17097820994780/Story1](https://public.tableau.com/app/profile/solomon8607/viz/Book1_17097820994780/Story1)

Stack: Databricks - Data extraction- data extraction, cleaning and ingestion, Azure Blob storage, Azure SQL database and Tableau for visualizations.

[Architecture](https://preview.redd.it/zxo0v4e76umc1.png?width=649&format=png&auto=webp&s=c17adfff1ae82b19e82df4171e71f845bd3c83be)

Github - [https://github.com/solo11/Data-engineering-project-1](https://github.com/solo11/Data-engineering-project-1)

The project uses web-scraping to extract Buffalo, NY realty data for the last 600 days from Zillow, [Realtor.com](https://Realtor.com) and Redfin. The dashboard provides visualizations and insights into the data. 

Any feedback is much appreciated, thank you!"
1b8k3em,How to learn Spark for interviews with minimal or no setup?,"Title. 
Preparing for interviews. Need to learn pyspark but need some platform so that I can focus on the learning without thinking too much about the set up. ",11,22,BinaryBass,2024-03-07 03:24:21,https://www.reddit.com/r/dataengineering/comments/1b8k3em/how_to_learn_spark_for_interviews_with_minimal_or/,0,False,False,False,False,"Title. 
Preparing for interviews. Need to learn pyspark but need some platform so that I can focus on the learning without thinking too much about the set up. "
1b894aq,Would you make this job switch?," 

I have 2 years experience in the midwest.

Current company (small consulting firm remote):

* **Salary**: 83k (expecting small raise in may)
* **PTO**: 15 days
* **Bonus**: 3-8% but has always been the bare minimum
* **Raises**: bare minimum because of low budget
* **401k**: 1% match
* **Other info:** Company has had 3 rounds of layoffs in the last year. Seems to be slightly struggling but my manager says our team shouldnt be worried.

New company (big health insurance remote):

* **Salary**: 77k (could be negotiable)
* **PTO**: 23 DAYS!!!!
* **Bonus**: 10%
* **Raises**: yearly depending on performance
* **401k**: 4% match + employer also contributes 1.25% of annual salary
* **Other info**: only one small layoff in the last 6 years. happened a month ago

Obviously the pay cut is not acceptable but I will try to negotiate. But the PTO they have is amazing. And I am sick of feeling like im on a sinking ship at the current company. This is more of a lateral move because I will be the same level at the new company as I am at the current company. What amount of base salary at the new company would convince you to make the switch?",9,16,spencedogg69,2024-03-06 19:43:18,https://www.reddit.com/r/dataengineering/comments/1b894aq/would_you_make_this_job_switch/,0,False,False,False,False," 

I have 2 years experience in the midwest.

Current company (small consulting firm remote):

* **Salary**: 83k (expecting small raise in may)
* **PTO**: 15 days
* **Bonus**: 3-8% but has always been the bare minimum
* **Raises**: bare minimum because of low budget
* **401k**: 1% match
* **Other info:** Company has had 3 rounds of layoffs in the last year. Seems to be slightly struggling but my manager says our team shouldnt be worried.

New company (big health insurance remote):

* **Salary**: 77k (could be negotiable)
* **PTO**: 23 DAYS!!!!
* **Bonus**: 10%
* **Raises**: yearly depending on performance
* **401k**: 4% match + employer also contributes 1.25% of annual salary
* **Other info**: only one small layoff in the last 6 years. happened a month ago

Obviously the pay cut is not acceptable but I will try to negotiate. But the PTO they have is amazing. And I am sick of feeling like im on a sinking ship at the current company. This is more of a lateral move because I will be the same level at the new company as I am at the current company. What amount of base salary at the new company would convince you to make the switch?"
1b8uryg,need recommendation,"hi guys, I am in a bit of a pickle at work. I will tell you my story and I would appreciate  if you could help me with identifying, if their reasons are plausible.

So I am a data analyst in an retail company (lets call it company J) with low data maturity. 

The company has server on-premise which is handled by our IT, there is also a ""processing"" server which is handled by ERP wannabe, but actually accounting software company (lets call it company A).

The issue is, before the company hired me, they depended on the company A to build reports in their software, but it takes them forever and/or don't do it because the load will be too big on the servers.

Now, I have to export excel files from company A's software and I am very limited in the amount of data. The software allows me to download sales of 3 months at a time. 

So I talked to company J's head of IT, he said - hey ask company A to give you access to their processing RDB server, let them give you ""read-only"" permission, should be all good.

I called company A they tell me - we will give you the access, but we won't be responsible for ANYTHING that happens to the servers. Even if we give you read-only, there is still a threat of SQL injections (they said something can be done with sql that still could be a threat). Another option is you set up a server for analytics and we will transfer the data you require.

I mentioned this to head of IT at company J, he was pissed, he said - they are full of sh#$t. If we add extra VM to our cluster, the load will be too big and will crash possibly. There are no such issues that company A said in modern day...

&#x200B;

Is the SQL injection threat really a bs? Is the advice of me getting a read-only permission to a processing server a bad advice?",7,6,Corvou,2024-03-07 13:34:23,https://www.reddit.com/r/dataengineering/comments/1b8uryg/need_recommendation/,1,False,False,False,False,"hi guys, I am in a bit of a pickle at work. I will tell you my story and I would appreciate  if you could help me with identifying, if their reasons are plausible.

So I am a data analyst in an retail company (lets call it company J) with low data maturity. 

The company has server on-premise which is handled by our IT, there is also a ""processing"" server which is handled by ERP wannabe, but actually accounting software company (lets call it company A).

The issue is, before the company hired me, they depended on the company A to build reports in their software, but it takes them forever and/or don't do it because the load will be too big on the servers.

Now, I have to export excel files from company A's software and I am very limited in the amount of data. The software allows me to download sales of 3 months at a time. 

So I talked to company J's head of IT, he said - hey ask company A to give you access to their processing RDB server, let them give you ""read-only"" permission, should be all good.

I called company A they tell me - we will give you the access, but we won't be responsible for ANYTHING that happens to the servers. Even if we give you read-only, there is still a threat of SQL injections (they said something can be done with sql that still could be a threat). Another option is you set up a server for analytics and we will transfer the data you require.

I mentioned this to head of IT at company J, he was pissed, he said - they are full of sh#$t. If we add extra VM to our cluster, the load will be too big and will crash possibly. There are no such issues that company A said in modern day...

&#x200B;

Is the SQL injection threat really a bs? Is the advice of me getting a read-only permission to a processing server a bad advice?"
1b8it55,Which courses compete with or are alternatives to Data Talk Zoomcamp?,"I'm not here to downplay Zoomcamp, just to explore new options and make a more informed decision.

The community strongly recommends Data Talk Zoomcamp for DE, and taking it is probably the next step I should take in my studies.

Are there any other alternatives that the community recommends at a similar level of:

- content quality
- preparation for real-world Data Engineering problems / jobs
- financial accessibility

Thanks",7,1,fjellen,2024-03-07 02:25:32,https://www.reddit.com/r/dataengineering/comments/1b8it55/which_courses_compete_with_or_are_alternatives_to/,1,False,False,False,False,"I'm not here to downplay Zoomcamp, just to explore new options and make a more informed decision.

The community strongly recommends Data Talk Zoomcamp for DE, and taking it is probably the next step I should take in my studies.

Are there any other alternatives that the community recommends at a similar level of:

- content quality
- preparation for real-world Data Engineering problems / jobs
- financial accessibility

Thanks"
1b8c1dk,Moving away from google sheets,"I recently moved to a new company where we are trying to create a pipeline for a downstream team that are going to use it for analytics. The upstream is the team directly involved with ""creating"" the dataset and their whole department works with sharepoint and google sheets. They create and maintain everything in excel manually because that's what they have been doing. So, for our pipeline we have to manually get the excel file from them  through slack or through email before we start the pipeline. I have tried having discussion with them where the file they create should be pushed to lake periodically but they don't want to move forward with it. We volunteered to build it from their side the integration to your lake but they don't want it. Now, my manager doesn't want me talking to them and do it as-is. Do you guys have experience how to deal with this situation and what can i do from side that can make this process a bit more automated? Any advice is appreciated. ",7,5,dekardar,2024-03-06 21:37:36,https://www.reddit.com/r/dataengineering/comments/1b8c1dk/moving_away_from_google_sheets/,0,False,False,False,False,"I recently moved to a new company where we are trying to create a pipeline for a downstream team that are going to use it for analytics. The upstream is the team directly involved with ""creating"" the dataset and their whole department works with sharepoint and google sheets. They create and maintain everything in excel manually because that's what they have been doing. So, for our pipeline we have to manually get the excel file from them  through slack or through email before we start the pipeline. I have tried having discussion with them where the file they create should be pushed to lake periodically but they don't want to move forward with it. We volunteered to build it from their side the integration to your lake but they don't want it. Now, my manager doesn't want me talking to them and do it as-is. Do you guys have experience how to deal with this situation and what can i do from side that can make this process a bit more automated? Any advice is appreciated. "
1b8vfdv,Favorite Python library?,What is your favorite Python library and why?,6,31,AMDataLake,2024-03-07 14:03:40,https://www.reddit.com/r/dataengineering/comments/1b8vfdv/favorite_python_library/,0,False,False,False,False,What is your favorite Python library and why?
1b8hrsh,"Iceberg + Dbt + Trino + Hive : modern, open-source data stack"," To provide a deeper understanding of how the modern, open-source data stack consisting of Iceberg, dbt, Trino, and Hive operates within a music streaming platform, let’s delve into the detailed workflow and benefits of each component. 

[https://medium.com/@stefentaime\_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597](https://medium.com/@stefentaime_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597)",4,1,Jealous_Ad6059,2024-03-07 01:37:40,https://www.reddit.com/r/dataengineering/comments/1b8hrsh/iceberg_dbt_trino_hive_modern_opensource_data/,0,False,False,False,False," To provide a deeper understanding of how the modern, open-source data stack consisting of Iceberg, dbt, Trino, and Hive operates within a music streaming platform, let’s delve into the detailed workflow and benefits of each component. 

[https://medium.com/@stefentaime\_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597](https://medium.com/@stefentaime_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597)"
1b8duhr,How to make this system design,"I am presently using [draw.io](https://draw.io), and I have been unable to locate icons resembling those in the provided image. I would greatly appreciate your assistance. Thank you.

https://preview.redd.it/qo3c75lflsmc1.png?width=1018&format=png&auto=webp&s=131a2a872005cd7f3543497c0d7c8e21b80f9505",3,2,wh1t3bl3,2024-03-06 22:49:17,https://www.reddit.com/r/dataengineering/comments/1b8duhr/how_to_make_this_system_design/,0,False,False,False,False,"I am presently using [draw.io](https://draw.io), and I have been unable to locate icons resembling those in the provided image. I would greatly appreciate your assistance. Thank you.

https://preview.redd.it/qo3c75lflsmc1.png?width=1018&format=png&auto=webp&s=131a2a872005cd7f3543497c0d7c8e21b80f9505"
1b8xyq6,Choice of Job Title,"My boss came to me and asked me what job title I wanted. Their suggestion was “Data Architect” but I’m leaning more towards “Data Engineer”.

To give background, I recently transitioned from mechanical engineering to this role within my company, where I am introducing a new PostgreSQL database and SQL Server data warehouse. I have been doing data modelling and am trying to get ER/Studio (or similar - open to suggestions) into practice at my company. I work with scientific data and am starting to write a Python script to import data from inline a software program into the database. In the near future I think I will be using tools like SSIS and Airflow.

What other job titles should I consider? Or should I just go with Data Engineer",2,7,Aromatic-Series-2277,2024-03-07 15:49:37,https://www.reddit.com/r/dataengineering/comments/1b8xyq6/choice_of_job_title/,0,False,False,False,False,"My boss came to me and asked me what job title I wanted. Their suggestion was “Data Architect” but I’m leaning more towards “Data Engineer”.

To give background, I recently transitioned from mechanical engineering to this role within my company, where I am introducing a new PostgreSQL database and SQL Server data warehouse. I have been doing data modelling and am trying to get ER/Studio (or similar - open to suggestions) into practice at my company. I work with scientific data and am starting to write a Python script to import data from inline a software program into the database. In the near future I think I will be using tools like SSIS and Airflow.

What other job titles should I consider? Or should I just go with Data Engineer"
1b912nc,From dbt to SQLMesh,"I'm excited to share my latest blog post, ""From dbt to SQLMesh"" 

Check it out here  [https://www.harness.io/blog/from-dbt-to-sqlmesh](https://www.harness.io/blog/from-dbt-to-sqlmesh)

We have been running sqlmesh in production in some way shape or form for a long time now, since the earliest versions of the tool. This post is focused on high level migration approach (which is the reason we were able to adopt quickly and drive innovation) & touches on the learnings and key takeaways too. Major points include simplification, no jinja (sqlglot macros instead creating valid and structurally correct sql syntax vs string interpolation free-for-all), better state management and simplified CICD+dev experience. Has been a rewarding experience and paid off manyfold in the quality, reliability, and maintainability of our data platform. And we are poised to scale extremely effectively. I hope this is useful for any folks looking for information on the topic or are just curious.",11,1,Academic_Ad_8747,2024-03-07 17:59:07,https://www.reddit.com/r/dataengineering/comments/1b912nc/from_dbt_to_sqlmesh/,1,False,False,False,False,"I'm excited to share my latest blog post, ""From dbt to SQLMesh"" 

Check it out here  [https://www.harness.io/blog/from-dbt-to-sqlmesh](https://www.harness.io/blog/from-dbt-to-sqlmesh)

We have been running sqlmesh in production in some way shape or form for a long time now, since the earliest versions of the tool. This post is focused on high level migration approach (which is the reason we were able to adopt quickly and drive innovation) & touches on the learnings and key takeaways too. Major points include simplification, no jinja (sqlglot macros instead creating valid and structurally correct sql syntax vs string interpolation free-for-all), better state management and simplified CICD+dev experience. Has been a rewarding experience and paid off manyfold in the quality, reliability, and maintainability of our data platform. And we are poised to scale extremely effectively. I hope this is useful for any folks looking for information on the topic or are just curious."
1b8ytdj,"Best practices for Terraform, AWS Glue, and CI/CD in data engineering","Hello everyone,

I'm working on streamlining our data engineering processes using AWS Glue, and I'd love to learn from others' experiences regarding:

* **Terraform for Glue:** Do you use Terraform to manage and provision your AWS Glue jobs and related infrastructure? If so, would you be willing to share some insights into your setup and any helpful tips?
* **CI/CD for Glue scripts:** How do you integrate CI/CD (e.g., GitLab CI/CD) to synchronize Python scripts with S3 buckets? Do you employ a monorepo strategy, or do you have a preferred alternative? What triggers your CI/CD pipelines (file changes, scheduled updates, etc.)?

I'd greatly appreciate any details on your best practices and any challenges you've overcome.

Thank you in advance for sharing your expertise!",3,6,Warsoco,2024-03-07 16:25:24,https://www.reddit.com/r/dataengineering/comments/1b8ytdj/best_practices_for_terraform_aws_glue_and_cicd_in/,1,False,False,False,False,"Hello everyone,

I'm working on streamlining our data engineering processes using AWS Glue, and I'd love to learn from others' experiences regarding:

* **Terraform for Glue:** Do you use Terraform to manage and provision your AWS Glue jobs and related infrastructure? If so, would you be willing to share some insights into your setup and any helpful tips?
* **CI/CD for Glue scripts:** How do you integrate CI/CD (e.g., GitLab CI/CD) to synchronize Python scripts with S3 buckets? Do you employ a monorepo strategy, or do you have a preferred alternative? What triggers your CI/CD pipelines (file changes, scheduled updates, etc.)?

I'd greatly appreciate any details on your best practices and any challenges you've overcome.

Thank you in advance for sharing your expertise!"
1b8xm1f,Data Transformation Comparison,"Tools like dbt are becoming a popular alternative to the legacy and platform tools like IBM DataStage or Azure Data Factory. Here is a quick comparison of five leading contenders to own the transformation stage in your data platform. (I leave off the dbt clones as largely derivative)  


||[Coalesce](https://coalesce.io)|[Coginiti](https://www.coginiti.co)|dbt Cloud|[Prophecy](https://www.prophecy.io)|[SQLMesh](https://sqlmesh.com)|
|:-|:-|:-|:-|:-|:-|
|Transform|LowCode|AaC|AaC|LowCode|AaC|
|Versioning|✅|✅|✅|✅|✅|
|Scheduling|✅|✅|✅|✅|✅|
|SQL|✅|✅|✅|✅|✅|
|Python|❌|❌|✅|❌|✅|
|DQ Tests|✅|✅|✅|❌|✅|
|Obj Store|❌|✅|❌|❌|❌|
|AI Assistant|❌|✅|❌|✅|❌|
|Orchestration API|✅|✅|✅|✅|❌|
|Data API|❌|✅|❌|❌|❌|
|Lineage|✅|✅|✅|✅|✅|
|Ad-Hoc Query|❌|✅|❌|❌|❌|
|Supported Platforms|Snowflake|Netezza, DB2 Warehouse, Databricks, Redshift, Athena, BigQuery, Yellowbrick, Starburst/Trino, Synapse, SQL Server, Postgres, Snowflake, Spark, Hive, Greenplum|Redshift, BigQuery, AlloyDB, Databricks, MSFT Fabric, Starburst/Trino, Postgres, Snowflake, Spark|Databricks|BigQuery, Databricks, DuckDB, MySQL, PostgreSQL, Redshift, Snowflake, Spark|

&#x200B;",2,0,Bazencourt,2024-03-07 15:35:36,https://www.reddit.com/r/dataengineering/comments/1b8xm1f/data_transformation_comparison/,0,False,False,False,False,"Tools like dbt are becoming a popular alternative to the legacy and platform tools like IBM DataStage or Azure Data Factory. Here is a quick comparison of five leading contenders to own the transformation stage in your data platform. (I leave off the dbt clones as largely derivative)  


||[Coalesce](https://coalesce.io)|[Coginiti](https://www.coginiti.co)|dbt Cloud|[Prophecy](https://www.prophecy.io)|[SQLMesh](https://sqlmesh.com)|
|:-|:-|:-|:-|:-|:-|
|Transform|LowCode|AaC|AaC|LowCode|AaC|
|Versioning|✅|✅|✅|✅|✅|
|Scheduling|✅|✅|✅|✅|✅|
|SQL|✅|✅|✅|✅|✅|
|Python|❌|❌|✅|❌|✅|
|DQ Tests|✅|✅|✅|❌|✅|
|Obj Store|❌|✅|❌|❌|❌|
|AI Assistant|❌|✅|❌|✅|❌|
|Orchestration API|✅|✅|✅|✅|❌|
|Data API|❌|✅|❌|❌|❌|
|Lineage|✅|✅|✅|✅|✅|
|Ad-Hoc Query|❌|✅|❌|❌|❌|
|Supported Platforms|Snowflake|Netezza, DB2 Warehouse, Databricks, Redshift, Athena, BigQuery, Yellowbrick, Starburst/Trino, Synapse, SQL Server, Postgres, Snowflake, Spark, Hive, Greenplum|Redshift, BigQuery, AlloyDB, Databricks, MSFT Fabric, Starburst/Trino, Postgres, Snowflake, Spark|Databricks|BigQuery, Databricks, DuckDB, MySQL, PostgreSQL, Redshift, Snowflake, Spark|

&#x200B;"
1b8sa5g,skyffel - prototype for generating Airbyte connectors,,2,1,phiandersson,2024-03-07 11:22:07,https://v.redd.it/fid5h8vbcwmc1,0,False,False,False,False,
1b8qxva,Dagster Cloud vs Dagster on GKE/GCP,"Coming from an airflow background. Just started using Dagster. What are the benefits to Dagster Cloud? Is one going to be significantly cheaper than the other? I know off the bat, that self hosting the webserver and all that will take a little more Ops work and config but I am up for it.

I'm reading that it enables ""Embedded ELT"", which is pretty much just dealing with the ingestion step. Currently I am using external queries in BigQuery to get the data in from Postgres (on Cloud SQL), which some might call a little hacky.",2,1,sinuspane,2024-03-07 09:59:16,https://www.reddit.com/r/dataengineering/comments/1b8qxva/dagster_cloud_vs_dagster_on_gkegcp/,1,False,False,False,False,"Coming from an airflow background. Just started using Dagster. What are the benefits to Dagster Cloud? Is one going to be significantly cheaper than the other? I know off the bat, that self hosting the webserver and all that will take a little more Ops work and config but I am up for it.

I'm reading that it enables ""Embedded ELT"", which is pretty much just dealing with the ingestion step. Currently I am using external queries in BigQuery to get the data in from Postgres (on Cloud SQL), which some might call a little hacky."
1b8m5e7,What are some things you enjoy about being a data engineer?,"What the title says. Currently a DA, looking to move into DE and looking for some perspectives. While DA is interesting, I think I will enjoy the Increased technical aspects of DE. What are some things that appeal to you?",2,0,Bureausaur,2024-03-07 05:07:07,https://www.reddit.com/r/dataengineering/comments/1b8m5e7/what_are_some_things_you_enjoy_about_being_a_data/,0,False,False,False,False,"What the title says. Currently a DA, looking to move into DE and looking for some perspectives. While DA is interesting, I think I will enjoy the Increased technical aspects of DE. What are some things that appeal to you?"
1b8hpn3,Replicate Remote Desktop MySQL instances to snowflake,"Working with a company who has several locations, each with a separate Remote Desktop, using the same software to manage their business. I’d like to replicate that data from the MySQL databases to a unified location in Snowflake. Seems like options include fivetran / air byte etc. are there any more custom solutions that are easy-ish, reliable and cost effective?",2,0,skiyogagolfbeer,2024-03-07 01:34:57,https://www.reddit.com/r/dataengineering/comments/1b8hpn3/replicate_remote_desktop_mysql_instances_to/,1,False,False,False,False,"Working with a company who has several locations, each with a separate Remote Desktop, using the same software to manage their business. I’d like to replicate that data from the MySQL databases to a unified location in Snowflake. Seems like options include fivetran / air byte etc. are there any more custom solutions that are easy-ish, reliable and cost effective?"
1b8ajea,Test coverage in dbt,"In dbt, I am trying to make sure that all our fact tables have relationship tests on their dimension fields.

Is there a tool which would do this? So far I've only seen packages which check the coverage for all fields, but I do not want to enforce that, only for the dim\_\*\_key patterned fields.

Do you have any similar solutions?",2,1,crepitation,2024-03-06 20:38:28,https://www.reddit.com/r/dataengineering/comments/1b8ajea/test_coverage_in_dbt/,1,False,False,False,False,"In dbt, I am trying to make sure that all our fact tables have relationship tests on their dimension fields.

Is there a tool which would do this? So far I've only seen packages which check the coverage for all fields, but I do not want to enforce that, only for the dim\_\*\_key patterned fields.

Do you have any similar solutions?"
1b88yj3,Data Warehouse - Fabric vs Opensource vs ?,"We are moving from a data lake of sorts, or better put, a bowl of spaghetti, to creating a new data warehouse. Our base system is  Dynamics 365 Business central. Open source or Fabric? Or is there a better alternative?",2,2,DutchN8G8,2024-03-06 19:37:03,https://www.reddit.com/r/dataengineering/comments/1b88yj3/data_warehouse_fabric_vs_opensource_vs/,1,False,False,False,False,"We are moving from a data lake of sorts, or better put, a bowl of spaghetti, to creating a new data warehouse. Our base system is  Dynamics 365 Business central. Open source or Fabric? Or is there a better alternative?"
1b885o0,How to read 10k small CSV files or 1 massive Parquet file efficiently (blogpost),"Hey r/dataengineering!

The Daft team published a blogpost about how Daft is able to handle ""adversarial"" file reading cases -- from 10k small CSVs to 1 giant Parquet file :)

See blogpost: [https://blog.getdaft.io/p/adversarial-file-reading-from-10000](https://blog.getdaft.io/p/adversarial-file-reading-from-10000)

I think it's pretty interesting and have come across this problem many times in my personal data engineering career so far. Some situations I've seen this happen:

1. 100k JSON/CSV files in AWS S3, dumped from an ingestion service every minute
2. 1 giant 1TB Parquet file dumped from an export process from a database

Historically, reading this type of data from the cloud has been really painful, and so the Daft team built in functionality to help you do this really efficiently. Happy to answer any questions about our approach!",2,0,get-daft,2024-03-06 19:06:09,https://www.reddit.com/r/dataengineering/comments/1b885o0/how_to_read_10k_small_csv_files_or_1_massive/,0,False,False,False,False,"Hey r/dataengineering!

The Daft team published a blogpost about how Daft is able to handle ""adversarial"" file reading cases -- from 10k small CSVs to 1 giant Parquet file :)

See blogpost: [https://blog.getdaft.io/p/adversarial-file-reading-from-10000](https://blog.getdaft.io/p/adversarial-file-reading-from-10000)

I think it's pretty interesting and have come across this problem many times in my personal data engineering career so far. Some situations I've seen this happen:

1. 100k JSON/CSV files in AWS S3, dumped from an ingestion service every minute
2. 1 giant 1TB Parquet file dumped from an export process from a database

Historically, reading this type of data from the cloud has been really painful, and so the Daft team built in functionality to help you do this really efficiently. Happy to answer any questions about our approach!"
1b916uw,Open Source Data Contract CLI to generate code or enforce the contract via testing,,1,0,simonharrer,2024-03-07 18:03:25,https://github.com/datacontract/cli,1,False,False,False,False,
1b90nap,How would you ETL JSONL data with different schemas in the same file?,"I have a dataset that consists of thousands of JSONL files (one JSON object per line). About 800GB in total, and 288 files of 15-50MB are added each day. Each file contains JSON objects with different schemas. Example:

&#x200B;

[Each 'LogAnalyticsCategory' has a different schema \(not visible in the picture, but you get the point\)](https://preview.redd.it/t6hmtz0i6ymc1.png?width=1332&format=png&auto=webp&s=84105e3453bfaed8019ec8f9dadebbeccc8d8d5c)

Eventually each category needs to go into a different destination table (sink = Postgres), but I'm unsure how to do this efficiently. I can do it easily with BigQuery, but given the volume of the data that will become too expensive, especially as the data will be queried for analytical purposes.  

My toolkit for now is basically limited to Python on a single machine. My source files are stored on GCS. Looping over the rows to figure out the schema of each line will probably be horribly slow. How do I best approach this (in the most simple way)?",1,1,unplannedmaintenance,2024-03-07 17:42:19,https://www.reddit.com/r/dataengineering/comments/1b90nap/how_would_you_etl_jsonl_data_with_different/,1,False,False,False,False,"I have a dataset that consists of thousands of JSONL files (one JSON object per line). About 800GB in total, and 288 files of 15-50MB are added each day. Each file contains JSON objects with different schemas. Example:

&#x200B;

[Each 'LogAnalyticsCategory' has a different schema \(not visible in the picture, but you get the point\)](https://preview.redd.it/t6hmtz0i6ymc1.png?width=1332&format=png&auto=webp&s=84105e3453bfaed8019ec8f9dadebbeccc8d8d5c)

Eventually each category needs to go into a different destination table (sink = Postgres), but I'm unsure how to do this efficiently. I can do it easily with BigQuery, but given the volume of the data that will become too expensive, especially as the data will be queried for analytical purposes.  

My toolkit for now is basically limited to Python on a single machine. My source files are stored on GCS. Looping over the rows to figure out the schema of each line will probably be horribly slow. How do I best approach this (in the most simple way)?"
1b90jmh,Databricks - Delta Live Tables Bronze to Silver,"Hey all!  


I've been around the forum for a while now but haven't ever posted anything. I've read through quite a few of the old posts on Databricks Delta Live Tables (DLT) and haven't quite gotten the answers that I'm after.   


Context: Our team is working on standing up a data lakehouse to better support analytics. We've made the decision to go with DLT to make use of the documentation it provides as well as it making SCD relatively simple. We are not able to use Unity Catalog but we're moving forward with DLT pointed at the hive metastore. I've written an initial pipeline that generates six dims and four fact tables. All of these tables are completely refreshed daily right now. I had done this to get something stood up in the short term while I continued learning DLT and to support some short-term data science efforts on our team.  


I now need to loop back and adjust my process to use SCD and ensure that I don't lose data when the source starts losing data (some of the data will disappear from my source later this year).  


In light of this I have a few questions I was really hoping the community may be able to help me with.  


1. My bronze tables at the moment are simple overwrites. We are considering making these append-only tables to support incremental and full loads as well as being able to track what data we actually processed. What do folks typically do here? Files are loaded from ADF and then processed into Bronze using Databricks.
2. I made an initial attempt at adding SCD-2 support to one of my dims in silver. It worked on the first run but the run on the following day errored out indicating that it detected an update or delete to one or more rows in the source table. My source table is a full overwrite and per examples I had seen online I was reading it with readStream. I switched it to a simple read and got an error stating that the view was not a streaming view. Does DLT apply\_changes require a streaming table for it's source? The documentation doesn't ever really indicate one way or the other but does talk about needing CDC data.
3. The final thing I haven't been able to fully wrap my head around was how fact tables are intended to be handled in DLT. I assume all dims should be SCD 1 or 2 but with regards to the fact table, am I intended to write a merge statement to continuously upsert records into the fact table (this way when my source stops sending me some data, I don't remove it with a full reload)?

Any help is greatly appreciated on this! Happy to clarify anything if necessary!",1,1,DisastrousCase3062,2024-03-07 17:38:23,https://www.reddit.com/r/dataengineering/comments/1b90jmh/databricks_delta_live_tables_bronze_to_silver/,1,False,False,False,False,"Hey all!  


I've been around the forum for a while now but haven't ever posted anything. I've read through quite a few of the old posts on Databricks Delta Live Tables (DLT) and haven't quite gotten the answers that I'm after.   


Context: Our team is working on standing up a data lakehouse to better support analytics. We've made the decision to go with DLT to make use of the documentation it provides as well as it making SCD relatively simple. We are not able to use Unity Catalog but we're moving forward with DLT pointed at the hive metastore. I've written an initial pipeline that generates six dims and four fact tables. All of these tables are completely refreshed daily right now. I had done this to get something stood up in the short term while I continued learning DLT and to support some short-term data science efforts on our team.  


I now need to loop back and adjust my process to use SCD and ensure that I don't lose data when the source starts losing data (some of the data will disappear from my source later this year).  


In light of this I have a few questions I was really hoping the community may be able to help me with.  


1. My bronze tables at the moment are simple overwrites. We are considering making these append-only tables to support incremental and full loads as well as being able to track what data we actually processed. What do folks typically do here? Files are loaded from ADF and then processed into Bronze using Databricks.
2. I made an initial attempt at adding SCD-2 support to one of my dims in silver. It worked on the first run but the run on the following day errored out indicating that it detected an update or delete to one or more rows in the source table. My source table is a full overwrite and per examples I had seen online I was reading it with readStream. I switched it to a simple read and got an error stating that the view was not a streaming view. Does DLT apply\_changes require a streaming table for it's source? The documentation doesn't ever really indicate one way or the other but does talk about needing CDC data.
3. The final thing I haven't been able to fully wrap my head around was how fact tables are intended to be handled in DLT. I assume all dims should be SCD 1 or 2 but with regards to the fact table, am I intended to write a merge statement to continuously upsert records into the fact table (this way when my source stops sending me some data, I don't remove it with a full reload)?

Any help is greatly appreciated on this! Happy to clarify anything if necessary!"
1b8zv2u,EL Tools for QuickBooks Enterprise,"All, 

My company currently contracts a vendor to handle the extract and load from our QuickBooks Enterprise database. We’ve searched time and again for an EL tool with QuickBooks Enterprise connectors, but have come up empty. 

I know Fivetran has a QuickBooks Online connector, but unfortunately, we’re not able to make the switch from QuickBooks Enterprise to QuickBooks Online (some functionality loss, supposedly). 

I figured before I give up the search, I’d come here and see if anyone had worked out a way to extract data from QuickBooks Enterprise? ",1,0,jayzfanacc,2024-03-07 17:11:45,https://www.reddit.com/r/dataengineering/comments/1b8zv2u/el_tools_for_quickbooks_enterprise/,1,False,False,False,False,"All, 

My company currently contracts a vendor to handle the extract and load from our QuickBooks Enterprise database. We’ve searched time and again for an EL tool with QuickBooks Enterprise connectors, but have come up empty. 

I know Fivetran has a QuickBooks Online connector, but unfortunately, we’re not able to make the switch from QuickBooks Enterprise to QuickBooks Online (some functionality loss, supposedly). 

I figured before I give up the search, I’d come here and see if anyone had worked out a way to extract data from QuickBooks Enterprise? "
1b8zeek,"Databricks SQL, Amazon Redshift, GBQ or Snowflake?","I am aware the answer will depend on many cases of what is the stack of the company, budget and familiarity but If you are working with one of these daily imagine you are selling/pitching each of these DW solutions here!",2,3,josejo9423,2024-03-07 16:53:31,https://www.reddit.com/r/dataengineering/comments/1b8zeek/databricks_sql_amazon_redshift_gbq_or_snowflake/,1,False,False,False,False,"I am aware the answer will depend on many cases of what is the stack of the company, budget and familiarity but If you are working with one of these daily imagine you are selling/pitching each of these DW solutions here!"
1b8z73y,Data engineering in a biopharma/biotech company,"Hello everyone,

I just recently joined a big pharma company to do some data engineering tasks (I'm basically more on the data science side, but have some experiences in building data pipelines).

In my previous company (a typical tech startup), building a data pipeline is relatively straightforward, most data sources either have data connectors or APIs so you can easily access the data and ingest them into the data lake/data warehouse, and you can use a lot of open-source tools for data transformation.

However, this time, I work with a lot of data generated from lab instruments, which typically need to be processed with specific software (typically the software made by the instruments' vendors). These software are either desktop app, and/or do not have any programmatic way (API or CLI) to access them. The raw data format is also often non standard text format (i.e. propriatary binary data format and there were no data parser available for most of them). Moreover, data processing is also not straightforward as often times it requires specific expertise in interpreting the meaningful information out of raw data, for example manual labeling and data selection from lab scientists.

Has anyone had any experience in building a data pipeline in this kinda settings? I wonder if I can pick up some successful strategies you guys have implemented previously.

Thank you.",2,2,mardian-octopus,2024-03-07 16:45:33,https://www.reddit.com/r/dataengineering/comments/1b8z73y/data_engineering_in_a_biopharmabiotech_company/,1,False,False,False,False,"Hello everyone,

I just recently joined a big pharma company to do some data engineering tasks (I'm basically more on the data science side, but have some experiences in building data pipelines).

In my previous company (a typical tech startup), building a data pipeline is relatively straightforward, most data sources either have data connectors or APIs so you can easily access the data and ingest them into the data lake/data warehouse, and you can use a lot of open-source tools for data transformation.

However, this time, I work with a lot of data generated from lab instruments, which typically need to be processed with specific software (typically the software made by the instruments' vendors). These software are either desktop app, and/or do not have any programmatic way (API or CLI) to access them. The raw data format is also often non standard text format (i.e. propriatary binary data format and there were no data parser available for most of them). Moreover, data processing is also not straightforward as often times it requires specific expertise in interpreting the meaningful information out of raw data, for example manual labeling and data selection from lab scientists.

Has anyone had any experience in building a data pipeline in this kinda settings? I wonder if I can pick up some successful strategies you guys have implemented previously.

Thank you."
1b8vzri,How can one practice (alone and for free) with DE tools or frameworks?,"I am referring to all those that involve any kind of subscription or pay-per-use formula. 
Any suggestions?
",2,2,hasty-beaver,2024-03-07 14:28:31,https://www.reddit.com/r/dataengineering/comments/1b8vzri/how_can_one_practice_alone_and_for_free_with_de/,1,False,False,False,False,"I am referring to all those that involve any kind of subscription or pay-per-use formula. 
Any suggestions?
"
1b8vlma,Google Cloud Storage Subscription,"Came across this service last week and thought what a great way to no longer having to setup an extra job to ingest messages from PubSub to GCS. It is supposed to support batching of messages (e.g. time window or file size). Anyone been able to get this to work as it's writing every single message to a file for me no matter what I configure it with?
",1,0,jroxtheworld,2024-03-07 14:11:13,https://www.reddit.com/r/dataengineering/comments/1b8vlma/google_cloud_storage_subscription/,1,False,False,False,False,"Came across this service last week and thought what a great way to no longer having to setup an extra job to ingest messages from PubSub to GCS. It is supposed to support batching of messages (e.g. time window or file size). Anyone been able to get this to work as it's writing every single message to a file for me no matter what I configure it with?
"
1b8uth2,"Snowplow community/Cloud - events forwarding, alternatives?","Hey all, does anyone have experience with the snowplow CDP ([https://snowplow.io](https://snowplow.io)), either community or cloud edition? My biggest question is how hard is event forwarding to e.g. Braze, Facebook Pixel, etc., when hosting the community version on own infrastructure (azure)?  


Any advice/learnings, or alternative solutions would be much appreciated! Thanks!",1,1,Kiria-Universal,2024-03-07 13:36:20,https://www.reddit.com/r/dataengineering/comments/1b8uth2/snowplow_communitycloud_events_forwarding/,0,False,False,False,False,"Hey all, does anyone have experience with the snowplow CDP ([https://snowplow.io](https://snowplow.io)), either community or cloud edition? My biggest question is how hard is event forwarding to e.g. Braze, Facebook Pixel, etc., when hosting the community version on own infrastructure (azure)?  


Any advice/learnings, or alternative solutions would be much appreciated! Thanks!"
1b8spuz,New Kid on the block?," 

Hey What’s up Reddit - this is a product launch announcement from me (Hugo) for Orchestra. Been a fan a while of the Reddit community and r/dataengineering even as a lurker so I’ll keep it brief

**What problem are we solving?**

Data Teams experience huge pains trying to create a single place to monitor the value of their data initiatives. As a result, we struggle to see what’s going on, and to be seen : communicating the value to business stakeholders is a daily struggle

**What technical challenges are there?**

We’re spending too much time building pipelines and infrastructure. For example, managing Kubernetes infrastructure for orchestration tools is overkill, data pipeline metadata is everywhere and can’t be aggregated, there is no nice UI for aggregating this metadata at a data product level. The list goes on..

**How do we do it?**

* Version-controlled, GUI-driven Data Pipeline Builder. Expand existing workflow orchestration tool specs to be conscious of data assets, data quality testing, and collect metadata, plus:
* Awesome UI for visualising all of this, plus:
* Custom alerting so you can stop using datadog / the thing you hacked together
* How does this compare to existing things?

The existing way to do this would probably be: Platform engineer + Workflow orchestration tool + Observability tool + custom alerting (and you still can’t really see how any metadata aggregated by data product). Not an easy undertaking for any team of any size

**The best bit**

We’re releasing a Free Tier so you can get all this for free as long as you don’t try to run 1 million pipelines and bankrupt us. Check it out here [https://app.getorchestra.io](https://app.getorchestra.io/) and on producthunt [https://www.producthunt.com/posts/orchestra-data-platform](https://www.producthunt.com/posts/orchestra-data-platform)

Questions concerns? Let me know in the comments!",1,0,engineer_of-sorts,2024-03-07 11:48:16,https://www.reddit.com/r/dataengineering/comments/1b8spuz/new_kid_on_the_block/,1,False,False,False,False," 

Hey What’s up Reddit - this is a product launch announcement from me (Hugo) for Orchestra. Been a fan a while of the Reddit community and r/dataengineering even as a lurker so I’ll keep it brief

**What problem are we solving?**

Data Teams experience huge pains trying to create a single place to monitor the value of their data initiatives. As a result, we struggle to see what’s going on, and to be seen : communicating the value to business stakeholders is a daily struggle

**What technical challenges are there?**

We’re spending too much time building pipelines and infrastructure. For example, managing Kubernetes infrastructure for orchestration tools is overkill, data pipeline metadata is everywhere and can’t be aggregated, there is no nice UI for aggregating this metadata at a data product level. The list goes on..

**How do we do it?**

* Version-controlled, GUI-driven Data Pipeline Builder. Expand existing workflow orchestration tool specs to be conscious of data assets, data quality testing, and collect metadata, plus:
* Awesome UI for visualising all of this, plus:
* Custom alerting so you can stop using datadog / the thing you hacked together
* How does this compare to existing things?

The existing way to do this would probably be: Platform engineer + Workflow orchestration tool + Observability tool + custom alerting (and you still can’t really see how any metadata aggregated by data product). Not an easy undertaking for any team of any size

**The best bit**

We’re releasing a Free Tier so you can get all this for free as long as you don’t try to run 1 million pipelines and bankrupt us. Check it out here [https://app.getorchestra.io](https://app.getorchestra.io/) and on producthunt [https://www.producthunt.com/posts/orchestra-data-platform](https://www.producthunt.com/posts/orchestra-data-platform)

Questions concerns? Let me know in the comments!"
1b8py81,Webinar: 5 Styles of Modern Data Integration,"I thought I would share this webinar. Data Virtuality is hosting a webinar on April 16 with The Eckerson Group.

Some of the key topics that will be covered include:

* The top five integration styles: data virtualization, ETL, CDC, ELT, and streaming
* Understand the pros and cons of each style and their ideal use cases
* Strategies to bridge the gap between business demands and IT realities
* How these integration styles enable architectural approaches like the data mesh, fabric, and lakehouse

Reserve your spot here - [https://go.datavirtuality.com/five-styles-of-modern-data-integration](https://go.datavirtuality.com/five-styles-of-modern-data-integration)",1,0,supergheo,2024-03-07 08:51:34,https://www.reddit.com/r/dataengineering/comments/1b8py81/webinar_5_styles_of_modern_data_integration/,0,False,False,False,False,"I thought I would share this webinar. Data Virtuality is hosting a webinar on April 16 with The Eckerson Group.

Some of the key topics that will be covered include:

* The top five integration styles: data virtualization, ETL, CDC, ELT, and streaming
* Understand the pros and cons of each style and their ideal use cases
* Strategies to bridge the gap between business demands and IT realities
* How these integration styles enable architectural approaches like the data mesh, fabric, and lakehouse

Reserve your spot here - [https://go.datavirtuality.com/five-styles-of-modern-data-integration](https://go.datavirtuality.com/five-styles-of-modern-data-integration)"
1b8nf7g,Native Unit Testing with dbt,,1,0,moderndatahack,2024-03-07 06:15:59,https://moderndatahack.substack.com/p/native-unit-testing-with-dbt,1,False,False,False,False,
1b8l2z1,dbt - Generating models from a DW,"Does anyone know if there is a way to clone your current DW into dbt models? 
I have years of regular SQL saved that I want to convert to DBT models for the docs.
This is my biggest barrier to adopting dbt so I'm sure there must be a package or Python script out there but I haven't found a way yet.
Needs to be either straight from the database or from saved SQL in git.
Would be cool if it grabs tableau dashboard sources from a server and saves them as exposures too.",1,0,snicky666,2024-03-07 04:12:01,https://www.reddit.com/r/dataengineering/comments/1b8l2z1/dbt_generating_models_from_a_dw/,1,False,False,False,False,"Does anyone know if there is a way to clone your current DW into dbt models? 
I have years of regular SQL saved that I want to convert to DBT models for the docs.
This is my biggest barrier to adopting dbt so I'm sure there must be a package or Python script out there but I haven't found a way yet.
Needs to be either straight from the database or from saved SQL in git.
Would be cool if it grabs tableau dashboard sources from a server and saves them as exposures too."
1b8gl8w,DuckDb + Azure functions + Azure blob storage,"Is anyone doing this? 

I need a scalable backend on azure for transformation where the ""large"" datasets are a couple of gigs. Jobs are on customer demand, so enough work that I want something that can scale, but maybe not quite enough to justify using something spark. 

I have a prototype of this that seems to work but before I build a whole system, curious if anyone is doing or has tried this and has any insight. I'm not a data person, just a dumb programmer. This is for part of the backend for a large SAAS system.",1,0,WMMMMMMMMMMMMMMMMMMW,2024-03-07 00:44:41,https://www.reddit.com/r/dataengineering/comments/1b8gl8w/duckdb_azure_functions_azure_blob_storage/,1,False,False,False,False,"Is anyone doing this? 

I need a scalable backend on azure for transformation where the ""large"" datasets are a couple of gigs. Jobs are on customer demand, so enough work that I want something that can scale, but maybe not quite enough to justify using something spark. 

I have a prototype of this that seems to work but before I build a whole system, curious if anyone is doing or has tried this and has any insight. I'm not a data person, just a dumb programmer. This is for part of the backend for a large SAAS system."
1b8fibk,European research groups,"Hi everyone, I’m a student that will graduate in a few months and I’m attracted by the research field. I was wonder if any of you know about research groups (universities/colleges or also professors) in EU or UK that are focused on the data engineering/management field. Topics that I’m passionate about are parallel computation, like anything about spark/dask/ray/etc., and something like the work of the DataLearning group (the only group that I know of).
Thank you all in advance ",1,1,itstimeoclock,2024-03-06 23:56:23,https://www.reddit.com/r/dataengineering/comments/1b8fibk/european_research_groups/,1,False,False,False,False,"Hi everyone, I’m a student that will graduate in a few months and I’m attracted by the research field. I was wonder if any of you know about research groups (universities/colleges or also professors) in EU or UK that are focused on the data engineering/management field. Topics that I’m passionate about are parallel computation, like anything about spark/dask/ray/etc., and something like the work of the DataLearning group (the only group that I know of).
Thank you all in advance "
1b8e24b,Scraping over 1200+ links.,"Hi everyone, I'm struggling with scraping over 1200 graphs from a server. For context, I'm using Playwright in Python, and I used Chromium to scrap data of a single graph (by the way, is a server in which I have to login with my username and password).  Scaling this for a 1200+ graphs is driving me crazy, I do not even know if this is even possible (for every graph I have to scrap, I launch either a new Chromium windows or tab).  Obviously the best way to do it is asking for the data itself to the webmaster, but this is off the table at the moment.

Any idea how can I do this? ",1,2,fmoralesh,2024-03-06 22:57:47,https://www.reddit.com/r/dataengineering/comments/1b8e24b/scraping_over_1200_links/,0,False,False,False,False,"Hi everyone, I'm struggling with scraping over 1200 graphs from a server. For context, I'm using Playwright in Python, and I used Chromium to scrap data of a single graph (by the way, is a server in which I have to login with my username and password).  Scaling this for a 1200+ graphs is driving me crazy, I do not even know if this is even possible (for every graph I have to scrap, I launch either a new Chromium windows or tab).  Obviously the best way to do it is asking for the data itself to the webmaster, but this is off the table at the moment.

Any idea how can I do this? "
1b8bscy,Spark Connect + EMR (Remote connectivity to spark clusters),"Background: With Spark 3.4, Spark Connect was released allowing individuals to remotely connect to remote spark clusters using the DataFrame API. They claim that it can be embedded in modern data apps, and even be able to use IDEs like VS Code. I think it's cool.  


Link: [https://spark.apache.org/docs/latest/spark-connect-overview.html](https://spark.apache.org/docs/latest/spark-connect-overview.html)

Question: I want to be able to do this with AWS EMR. Did anyone try this and was successful or mind sharing pointers on how to set it up? ",1,0,Top-Net-1111,2024-03-06 21:27:52,https://www.reddit.com/r/dataengineering/comments/1b8bscy/spark_connect_emr_remote_connectivity_to_spark/,1,False,False,False,False,"Background: With Spark 3.4, Spark Connect was released allowing individuals to remotely connect to remote spark clusters using the DataFrame API. They claim that it can be embedded in modern data apps, and even be able to use IDEs like VS Code. I think it's cool.  


Link: [https://spark.apache.org/docs/latest/spark-connect-overview.html](https://spark.apache.org/docs/latest/spark-connect-overview.html)

Question: I want to be able to do this with AWS EMR. Did anyone try this and was successful or mind sharing pointers on how to set it up? "
1b8bjwt,Visual Studio Data Tools,"Hi. 

I work at a company that despite being relatively large, is about twenty years behind the times. I’m talking most folks have been at this org their entire careers and they’ve never even dealt with pivot tables. 

All of that to say this: there’s barely any infrastructure at all. I need to build some but the only software that’s compatible with what is in use is Visual Studio. I was hoping to use SSIS. We don’t need a lot of things done, just a simple small data warehouse. 

Thing is… I’ve installed visual studio 2022. I have a license. I’ve gotten SSDT & SSIS installed and I can’t create a new SSIS project. 

The internet seems to waiver between SSIS being unsupported in VStudio and the need to use Azure. We’re not using Azure any where else. 

What are my options?  Does Microsoft want me to hate everything? ",1,1,TodosLosPomegranates,2024-03-06 21:18:35,https://www.reddit.com/r/dataengineering/comments/1b8bjwt/visual_studio_data_tools/,0,False,False,False,False,"Hi. 

I work at a company that despite being relatively large, is about twenty years behind the times. I’m talking most folks have been at this org their entire careers and they’ve never even dealt with pivot tables. 

All of that to say this: there’s barely any infrastructure at all. I need to build some but the only software that’s compatible with what is in use is Visual Studio. I was hoping to use SSIS. We don’t need a lot of things done, just a simple small data warehouse. 

Thing is… I’ve installed visual studio 2022. I have a license. I’ve gotten SSDT & SSIS installed and I can’t create a new SSIS project. 

The internet seems to waiver between SSIS being unsupported in VStudio and the need to use Azure. We’re not using Azure any where else. 

What are my options?  Does Microsoft want me to hate everything? "
1b88no5,Help to find the best way,"I'm trying to find a solution to determine the best way to deliver data to my Curated/Data Warehouse layer. Context:

* There are thousands of pages(html) saved as text in the raw layer.
* The daily ingestion isn't significant, around 10-15 files per day.
* Each txt file is a page 

I have conducted some studies, and the information I need to extract from these HTML files generate a nested json with 4 levels.  
Ship(only 1) > Owners(many) > Containers(many) > orders(many)

My question is... How do you guys think would be the best approach to deliver this information at the end of the pipeline?

raw(html) > refined(parquet) > stage table > facts (ship, containers, orders) > analysis OR raw(html) > refined (parquet - ship, containers, orders) > facts (ship, containers, orders) > analysis OR raw(html) > refined (parquet) > curated(OBT) > analysis",1,0,ltofanelli,2024-03-06 19:25:25,https://www.reddit.com/r/dataengineering/comments/1b88no5/help_to_find_the_best_way/,1,False,False,False,False,"I'm trying to find a solution to determine the best way to deliver data to my Curated/Data Warehouse layer. Context:

* There are thousands of pages(html) saved as text in the raw layer.
* The daily ingestion isn't significant, around 10-15 files per day.
* Each txt file is a page 

I have conducted some studies, and the information I need to extract from these HTML files generate a nested json with 4 levels.  
Ship(only 1) > Owners(many) > Containers(many) > orders(many)

My question is... How do you guys think would be the best approach to deliver this information at the end of the pipeline?

raw(html) > refined(parquet) > stage table > facts (ship, containers, orders) > analysis OR raw(html) > refined (parquet - ship, containers, orders) > facts (ship, containers, orders) > analysis OR raw(html) > refined (parquet) > curated(OBT) > analysis"
1b8y2ku,I need help with my CV. Does anyone offer CV building services focus on tech/data?,Would love some recommendations for tech/data focused career consultants to have professional feedback and maybe mock ups on alternative CV layouts.,0,0,data-punk,2024-03-07 15:53:52,https://www.reddit.com/r/dataengineering/comments/1b8y2ku/i_need_help_with_my_cv_does_anyone_offer_cv/,0,False,False,False,False,Would love some recommendations for tech/data focused career consultants to have professional feedback and maybe mock ups on alternative CV layouts.
