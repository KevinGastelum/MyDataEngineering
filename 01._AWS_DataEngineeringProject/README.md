# AWS End to End Data Engineering Project

## Real Time Reddit Data Extraction --> Data Warehousing --> Data Modeling --> Data Pipelines

End to End AWS project to extract, transform, and load (ETL) data from Reddit into a Redshift data warehouse. This pipeline integrates multiple technologies to ensure efficient data handling and storage.

<img src="images\RedditDataEngineering-.png">

## Technologies Used

- **Data Extraction**: Reddit API
- **Workflow Automation**: Apache Airflow, Celery
- **Database Management**: PostgreSQL
- **Cloud Storage**: Amazon S3
- **Data Transformation**: AWS Glue, Lambda
- **Query Service**: Amazon Athena
- **Data Warehousing**: Amazon Redshift
- **Data Visualization**:

## Data Pipeline

- **Automated Data Processing Workflow**: Utilizing Apache Airflow and Celery for data processing.
- **Data Storage**: PostgreSQL and Amazon S3 for data storage.
- **Data Transformation**: Integrates AWS Glue, Lambda and Amazon Athena for effective data transformation and querying.
- **Scalable Data Warehousing**: Utilizes Amazon Redshift for a high-performance data warehousing solution.

## Conclusion

This project showcases my ability to integrate various technologies to create a robust and scalable data pipeline. It demonstrates my expertise in handling big data and my commitment to delivering efficient and reliable data solutions.

<!--

Take screensshots of Docker/Airflow, AWS EC2/S3, SQL/Celery, Glue/Lambda, Athena/Redshift, Visuals

-->

<!--
Docker Commands =

docker compose up -d --build
docker compose up -d

docker exec -it


--Fresh Start steps
Set up VENV - Conda
run reqs.txt to install all required packages
pull in config.conf settings , data, logs, plugins, tests
run docker build

-->
